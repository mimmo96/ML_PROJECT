import itertools
import numpy as np
import Matrix_io
import neural_network
import math
import ensemble
from Model_Selection import ThreadPool_average

def cv_k_fold(grid, epochs, training_set, test_set, type_problem, k_fold = 4):
    num_training = -1
    trials_per_model = 10
    #divide dataset into K distinct and equal D_1, ..., D_K
    size_validation = training_set.input().shape[0] // k_fold
    
    #grid = list(itertools.product(vectors_units, vector_alfa, vector_lambda, vector_learning_rate, batch_array, function_hidden, function_output, type_weight))
    size_top_NN = 10 
    top_NN = ensemble.ensemble()
    
    for hyperparameter in grid:
        #iperparametri
        batch_size=hyperparameter[0]
        function=hyperparameter[1]
        fun_out=hyperparameter[2]
        units=hyperparameter[3]
        learning_rate=hyperparameter[4]
        alfa=hyperparameter[5]
        v_lambda=hyperparameter[6]
        weig=hyperparameter[7]
        num_training += 1
        #it will cointain K models generated by k partition of TR
        NN_k_fold = []
        for k in range(k_fold):
            
            if k == k_fold-1:
                last_set = True
            else:
                last_set = False
            #create training and validation set for Kth iteration of k_fold
            training_k, validation_k = training_set.create_fold(k*size_validation, (k+1)*size_validation, last_set)
            
            #for each model, train "trials_for_model" times with different initialization weights
            NN_trials = np.empty(trials_per_model, neural_network.neural_network)
            mean_mse, mean_mee, model = ThreadPool_average(type_problem,fun_out,training_k,validation_k, batch_size, epochs, num_training,units, alfa, v_lambda, learning_rate, len(units) - 2, weig, function)
            
            #for i in range(trials_per_model):
                #NN_trials[i] = neural_network.neural_network(hyperparameter[0], hyperparameter[1], hyperparameter[2], hyperparameter[3], (np.size(hyperparameter[0])-1), hyperparameter[5], hyperparameter[6], hyperparameter[7], type_problem)
                #NN_trials[i].trainig(training_k, validation_k, hyperparameter[4], epochs, num_training)
            
            #NN_k_fold[k] = list of tuple (neaural network, mean_mse, mean_mee, num_training) it contains mean mse btw NN_trials 
            # and the best model (inside NN_trials) respect to its mse
            #model, mean_mse, mean_mee = mean_NN(NN_trials, validation_k)
            NN_k_fold.append(ensemble.stat_model(model, mean_mse, mean_mee, num_training))
        
        #best_NN_fold = tuple (neaural network, mean_mse, mean_mee, num_training) it contains best model btw k models generated 
        # and the mean mse btw k models
        NN_k_fold = ensemble.ensemble(NN_k_fold, [], k_fold)
        best_NN_k_fold = NN_k_fold.mean_loss() 
        
        top_NN.k_is_in_top(best_NN_k_fold)
    
    #retraining with early stopping
    #
    #           write here
    #
    #   
    return top_NN


#return the best NN inside array_of_NN, mean mse, mean dev standard 
def mean_NN(array_of_NN, data_set):
    #AGGIUNGERE DEVIAZIONE STANDARD
    mean_mse = 0
    mean_mee = 0
    mean_accuracy = 0
    best_mse = + math.inf
    best_mee = + math.inf
    
    for NN in array_of_NN:
        accuracy, mse, mee = NN.validation(data_set, penalty_term = 0)
        
        if mee < best_mee:
            best_mee = mee
            best_mse = mse
            best_NN = NN

        mean_mee += mee
        mean_mse += mse 
        mean_accuracy += accuracy
    
    mean_mse /= np.size(array_of_NN)
    mean_mee /= np.size(array_of_NN)
    mean_accuracy /= np.size(array_of_NN)

    return best_NN, mean_mse, mean_mee


