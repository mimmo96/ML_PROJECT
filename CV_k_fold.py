import itertools
import numpy as np
import Matrix_io
import neural_network
import math
def cv_k_fold(vector_alfa, vector_learning_rate, vector_lambda, vectors_units, training_set, test_set, batch_array, epochs,function_hidden,function_output, type_weight,type_problem, k_fold = 5):
    #sistemare questo paramentro:##############################################################################################################
    num_training = 0
    ############################################################################################################################################
    trials_per_model = 10
    #divide dataset into K distinct and equal D_1, ..., D_K
    size_validation = training_set.input().shape[0] // k_fold
    
    grid = list(itertools.product(vectors_units, vector_alfa, vector_lambda, vector_learning_rate, batch_array, function_hidden, function_output, type_weight))
    size_top_NN = 10 
    top_NN = np.empty(size_top_NN, tuple) 
    
    for hyperparameter in grid:
        for k in range(k_fold):
            #cointain K models generated by k partition of TR
            NN_k_fold = np.empty(k_fold, tuple)
            if k == k_fold-1:
                last_set = True
            else:
                last_set = False

            training_k, validation_k = training_set.create_fold(k*size_validation, (k+1)*size_validation, last_set)
            
            #for each model, train "trials_for_model" times with different initialization weights
            NN_trials = np.empty(trials_per_model, neural_network.neural_network)
            for i in range(trials_per_model):
                NN_trials[i] = neural_network.neural_network(hyperparameter[0], hyperparameter[1], hyperparameter[2], hyperparameter[3], (np.size(hyperparameter[0])-1), function_hidden, function_output, type_weight, type_problem)
                NN_trials[i].trainig(training_k, validation_k, hyperparameter[4], epochs, num_training)
            #NN_k_fold[k] = tuple (neaural network, mean_error, dev_standard)
            NN_k_fold[k] = mean_NN(NN_trials, validation_k)
        ########################################################################################    
        best_NN_k_fold = mean_NN_k_fold(NN_trials, validation_k) #STESSO MODO????????????????????????????????
        ##################################################################################
        if k_is_in_top(top_NN, best_NN_k_fold, size_top_NN):
            top_NN.append(best_NN_k_fold)
    
    #retraining with early stopping
    #
    #           write here
    #
    #   
    return top_NN


#return the best NN inside array_of_NN, mean error, mean dev standard 
def mean_NN(array_of_NN, data_set):
    #AGGIUNGERE DEVIAZIONE STANDARD
    mean_error = 0
    mean_accuracy = 0
    best_error = + math.inf
    
    for NN in array_of_NN:
        accuracy, error = NN.validation(data_set, penalty_term = 0)
        
        if error < best_error:
            best_error = error
            best_NN = NN

        mean_error += error 
        mean_accuracy += accuracy
    
    mean_error /= np.size(array_of_NN)
    mean_accuracy /= np.size(array_of_NN)

return best_NN, mean_error


def mean_NN_k_fold(tuples_NN):
    #AGGIUNGERE DEVIAZIONE STANDARD
    mean_error = 0
    best_error = + math.inf

    for tuple_NN in tuples_NN:
        
        if best_error < tuple_NN[1]:
            best_error = tuple_NN[1]
            best_NN = tuple_NN[0]

        mean_error += tuple_NN[1] 
    
    mean_error /= np.size(tuples_NN)
    return best_NN, mean_error

def k_is_in_top(top_NN, best_NN_k_fold, limit):
    
    if top_NN.size < limit:
        return True
    
    top_NN = sorted(top_NN, key=lambda x:x[1])
    
    if top_NN[-1][1] > best_NN_k_fold[1]
        top_NN.pop()
        return True
    
    return False

